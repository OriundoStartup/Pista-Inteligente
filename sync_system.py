import os
import sys
import webbrowser
import time
import socket
import subprocess
import sqlite3
from datetime import datetime
from src.etl.etl_pipeline import HipicaETL
from src.models.train_v2 import HipicaLearner
from src.models.data_manager import obtener_analisis_jornada, calcular_todos_patrones
import json
from pathlib import Path

import numpy as np
import argparse

def _safe_int(val):
    """Convierte de forma segura a int, manejando bytes/blobs de numpy/sqlite."""
    try:
        if val is None: return 0
        if isinstance(val, (bytes, bytearray)):
            # Asumir little-endian para enteros de 64 bits si viene de numpy
            return int.from_bytes(val[:8], byteorder='little')
        return int(float(str(val)))
    except:
        return 0

def is_port_in_use(port):
    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
        return s.connect_ex(('localhost', port)) == 0

def kill_server_on_port(port):
    """Mata el proceso que ocupa el puerto especificado (Windows/Linux)"""
    import signal
    print(f"üîÑ Intentando liberar puerto {port}...")
    try:
        if sys.platform == 'win32':
            # Buscar PID usando netstat
            cmd = f'netstat -ano | findstr :{port}'
            result = subprocess.run(cmd, shell=True, capture_output=True, text=True)
            lines = result.stdout.strip().split('\n')
            for line in lines:
                parts = line.split()
                if len(parts) > 4:
                    pid = parts[-1]
                    print(f"   -> Matando proceso PID {pid}...")
                    subprocess.run(f'taskkill /F /PID {pid}', shell=True, capture_output=True)
        else:
            # Linux/Mac logic (fuser/lsof could be used, or simple killall python)
            pass
    except Exception as e:
        print(f"‚ö†Ô∏è No se pudo liberar puerto: {e}")

class CustomJSONEncoder(json.JSONEncoder):
    """Encoder personalizado para manejar tipos de Numpy"""
    def default(self, obj):
        if isinstance(obj, (np.int_, np.intc, np.intp, np.int8,
                            np.int16, np.int32, np.int64, np.uint8,
                            np.uint16, np.uint32, np.uint64)):
            return int(obj)
        elif isinstance(obj, (np.float_, np.float16, np.float32, np.float64)):
            return float(obj)
        elif isinstance(obj, (np.ndarray,)):
            return obj.tolist()
        return super(CustomJSONEncoder, self).default(obj)

def save_predictions_to_db(analisis, update_mode=True):
    """
    Guarda o actualiza las predicciones de manera inteligente (UPSERT manual).
    Verifica existencia para decidir entre UPDATE o INSERT, evitando duplicados.
    """
    try:
        db_path = 'data/db/hipica_data.db'
        conn = sqlite3.connect(db_path)
        cursor = conn.cursor()
        
        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        rows_to_insert = []
        rows_to_update = []
        
        print("   üîç Verificando existencia de predicciones para UPSERT...")
        
        # Cache de existencia para reducir SELECTs individuales
        # Clave: (fecha, hipodromo, nro_carrera, numero_caballo)
        # Valor: True
        # Cargar todas las predicciones futuras existentes en memoria (son pocas)
        cursor.execute("SELECT fecha_carrera, hipodromo, nro_carrera, numero_caballo FROM predicciones WHERE fecha_carrera >= date('now', 'localtime')")
        existing_preds = set()
        for row in cursor.fetchall():
            # (fecha, hipodromo, carrera, caballo)
            existing_preds.add((row[0], row[1], int(row[2]), int(row[3])))

        for carrera in analisis:
            fecha_carrera = carrera.get('fecha')
            hipodromo = carrera.get('hipodromo')
            nro_carrera = _safe_int(carrera.get('carrera'))
            predicciones = carrera.get('predicciones', [])
            
            for idx, pred in enumerate(predicciones, 1):
                if isinstance(pred, dict):
                    numero = _safe_int(pred.get('numero', 0))
                    puntaje_ia = pred.get('puntaje_ia', 0.0)
                    prob_ml_val = pred.get('prob_ml', '0.0')
                    if isinstance(prob_ml_val, str):
                         prob_ml = float(prob_ml_val.replace('%', ''))
                    else:
                         prob_ml = float(prob_ml_val)
                    
                    # IDs
                    caballo_nombre = pred.get('caballo', '')
                    jinete_nombre = pred.get('jinete', '')
                    
                    # Resolver IDs
                    caballo_id = None
                    cursor.execute("SELECT id FROM caballos WHERE nombre = ?", (caballo_nombre,))
                    res = cursor.fetchone()
                    if res: caballo_id = res[0]
                    
                    jinete_id = None
                    cursor.execute("SELECT id FROM jinetes WHERE nombre = ?", (jinete_nombre,))
                    res = cursor.fetchone()
                    if res: jinete_id = res[0]
                    
                    metadata = json.dumps({
                        'caballo': caballo_nombre,
                        'jinete': jinete_nombre,
                        'updated_at': timestamp
                    }, ensure_ascii=False)
                    
                    # Decisi√≥n UPSERT
                    key = (fecha_carrera, hipodromo, nro_carrera, numero)
                    
                    if key in existing_preds:
                        # UPDATE
                        rows_to_update.append((
                            puntaje_ia,
                            prob_ml,
                            idx, # ranking
                            metadata,
                            timestamp,
                            fecha_carrera,
                            hipodromo,
                            nro_carrera,
                            numero
                        ))
                    else:
                        # INSERT
                        rows_to_insert.append((
                            timestamp,
                            fecha_carrera,
                            hipodromo,
                            nro_carrera,
                            numero,
                            caballo_id,
                            jinete_id,
                            puntaje_ia,
                            prob_ml,
                            idx,  # ranking
                            metadata
                        ))
        
        # Ejecutar batch
        if rows_to_insert:
            cursor.executemany('''
                INSERT INTO predicciones 
                (fecha_generacion, fecha_carrera, hipodromo, nro_carrera, numero_caballo, 
                 caballo_id, jinete_id, puntaje_ia, prob_ml, ranking_prediccion, metadata)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            ''', rows_to_insert)
            print(f"   üíæ Insertadas {len(rows_to_insert)} nuevas predicciones.")

        if rows_to_update:
            cursor.executemany('''
                UPDATE predicciones
                SET puntaje_ia=?, prob_ml=?, ranking_prediccion=?, metadata=?, fecha_generacion=?
                WHERE fecha_carrera=? AND hipodromo=? AND nro_carrera=? AND numero_caballo=?
            ''', rows_to_update)
            print(f"   üîÑ Actualizadas {len(rows_to_update)} predicciones existentes.")
            
        conn.commit()
        conn.close()
        return True
        
    except Exception as e:
        print(f"   ‚ö†Ô∏è Error en UPSERT predicciones: {e}")
        import traceback
        traceback.print_exc()
        return False

def precalculate_predictions(update_mode=False):
    """
    Pre-calcula predicciones y las guarda en cache y base de datos.
    Args:
        update_mode: Si es True, indica que estamos actualizando por nuevos resultados (UPDATE en BD).
                     Si es False, es una carga normal o nueva (INSERT).
    """
    print(f"üìä Pre-calculando predicciones... (Modo Actualizaci√≥n: {update_mode})")
    try:
        cache_path = Path("data/cache_analisis.json")
        
        # Siempre queremos refrescar el cache JSON para la vista
        if cache_path.exists():
            cache_path.unlink()
        

        
        print("   üîÑ Obteniendo predicciones frescas (SQL Optimizado)...")
        # Esto ahora usa cargar_programa(solo_futuras=True) internamente
        analisis = obtener_analisis_jornada()
        
        if not analisis:
            print("   ‚ö†Ô∏è No hay carreras futuras para analizar.")
            return True # No es error, solo no hay datos
        
        # Debug
        fechas = set(c.get('fecha') for c in analisis)
        print(f"   üìÖ Fechas analizadas: {sorted(fechas)}")
        
        # Guardar o Actualizar en DB
        print(f"   üíæ {'Actualizando' if update_mode else 'Guardando'} en Base de Datos...")
        save_predictions_to_db(analisis, update_mode=update_mode)
        
        # Serializar para Cache JSON (Frontend)
        analisis_serializable = []
        for carrera in analisis:
            carrera_dict = carrera.copy()
            if hasattr(carrera_dict['caballos'], 'to_dict'):
                carrera_dict['caballos'] = carrera_dict['caballos'].to_dict('records')
            if hasattr(carrera_dict['predicciones'], 'to_dict'):
                carrera_dict['predicciones'] = carrera_dict['predicciones'].to_dict('records')
            analisis_serializable.append(carrera_dict)

        cache_path.parent.mkdir(exist_ok=True, parents=True)
        with open(cache_path, 'w', encoding='utf-8') as f:
            json.dump(analisis_serializable, f, ensure_ascii=False, indent=2, cls=CustomJSONEncoder)
        
        print(f"   ‚úÖ Cache JSON regenerado en {cache_path}")
        return True
    except Exception as e:
        import traceback
        traceback.print_exc()
        print(f"‚ùå Error al pre-calcular predicciones: {e}")
        return False

def precalculate_patterns(update_mode=False):
    """
    Pre-calcula patrones de resultados y los guarda en cache JSON.
    """
    print(f"üìä Pre-calculando patrones de resultados... (Modo Actualizaci√≥n: {update_mode})")
    try:
        cache_path = Path("data/cache_patrones.json")
        
        # Siempre refrescamos cache
        if cache_path.exists():
            cache_path.unlink()
            
        print("   üîÑ Calculando todos los patrones...")
        patrones = calcular_todos_patrones()
        
        if not patrones:
            print("   ‚ö†Ô∏è No se encontraron patrones.")
            # Crear archivo vac√≠o v√°lido con metadata
            data = {
                'last_updated': datetime.now().strftime('%d-%m-%Y %H:%M'),
                'patterns': []
            }
            with open(cache_path, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2, cls=CustomJSONEncoder)
            return True
            
        print(f"   ‚úÖ Encontrados {len(patrones)} patrones.")
        
        data = {
            'last_updated': datetime.now().strftime('%d-%m-%Y %H:%M'),
            'patterns': patrones
        }
        
        cache_path.parent.mkdir(exist_ok=True, parents=True)
        with open(cache_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2, cls=CustomJSONEncoder)
        
        print(f"   ‚úÖ Cache Patrones JSON regenerado en {cache_path}")
        return True
    except Exception as e:
        import traceback
        traceback.print_exc()
        print(f"‚ùå Error al pre-calcular patrones: {e}")
        return False

def check_git_changes():
    """Retorna True si hay cambios pendientes en git (c√≥digo modificado)."""
    try:
        # Check staged + unstaged + untracked
        # git status --porcelain es la forma m√°s robusta
        r = subprocess.run(["git", "status", "--porcelain"], capture_output=True, text=True)
        if r.stdout.strip(): 
            return True
        return False
    except:
        return False

def main(force_sync=False):
    """
    Sistema de sincronizaci√≥n principal optimizado.
    Args:
        force_sync: Si es True, fuerza re-procesamiento completo ignorando tracking
    """
    print("""
    ==================================================
       SISTEMA DE HIPICA INTELIGENTE - SYNC V2.1
    ==================================================
    DETECTANDO ARCHIVOS EN /exports...
    """)
    
    start_time = time.time()
    
    try:
        # 1. Inicializar y Correr ETL
        print("\n[PASO 1/3] Ejecutando ETL (Carga de Datos)...")
        etl = HipicaETL()
        archivos_nuevos = etl.run(force_reprocess=force_sync)
        
        # Verificar si hay cambios que requieran actualizaci√≥n (Datos o C√≥digo)
        cambios_codigo = check_git_changes()
        hay_cambios = (archivos_nuevos > 0) or cambios_codigo
        
        if cambios_codigo:
             print("\nüìù Detectados cambios en el c√≥digo fuente. Se proceder√° al despliegue.")
        
        if not hay_cambios and not force_sync:
            print("\n‚úÖ SISTEMA ACTUALIZADO - No hay cambios nuevos (ni datos ni c√≥digo).")
            print("üìä Los datos, modelos y predicciones est√°n vigentes.")
            print(f"\n‚è±Ô∏è Tiempo total: {time.time() - start_time:.2f} segundos")
            return
        
        print(f"\nüîÑ Detectados {archivos_nuevos} archivo(s) nuevo(s). Actualizando sistema...")
        
        # 2. Entrenar/Actualizar Modelos de IA
        # print("\n[PASO 2/3] Entrenando Modelos de IA (HipicaLearner)...")
        # try:
        #     # learner = HipicaLearner()
        #     # learner.train()
        #     print("‚ö†Ô∏è Entrenamiento autom√°tico DESACTIVADO por seguridad (Plan de Correcci√≥n).")
        #     # print("‚úÖ Modelos de Inteligencia Artificial actualizados correctamente.")
        # except Exception as e_ml:
        #     print(f"‚ö†Ô∏è Advertencia: No se pudo entrenar el modelo de IA: {e_ml}")
        #     print("   -> El sistema continuar√° con los modelos anteriores si existen.")

        elapsed = time.time() - start_time
        print(f"\n‚úÖ SINCRONIZACI√ìN COMPLETADA en {elapsed:.2f} segundos.")
        print(" -> La Base de Datos ha sido actualizada.")
        print(" -> Modelos de Predicci√≥n (V3 - HistGradientBoosting): ACTUALIZADOS.")
        
        # 2.5 Pre-calcular Predicciones (Cache + BD)
        # Activar update_mode=True para aprovechar la l√≥gica de Re-Predicci√≥n (Update)
        # La funci√≥n internamente debe manejar inserts para nuevos programas (Pendiente de ajuste en save)
        print("\n[PASO 2.5/3] Recalculando Predicciones Futuras (Update + Insert)...")
        precalculate_predictions(update_mode=True)
        
        # 2.6 Pre-calcular Patrones
        print("\n[PASO 2.6/3] Recalculando Patrones de Resultados...")
        precalculate_patterns(update_mode=True)

        # 3. Deploy a Cloud Run (Firebase)
        print("\n[PASO 3/3] Desplegando a Cloud Run (Firebase)...")
        deploy_to_cloud_run()

    except Exception as e:
        print(f"\n‚ùå ERROR CR√çTICO: {e}")
        import traceback
        traceback.print_exc()
        input("Presiona Enter para salir...")

def deploy_to_cloud_run():
    """Despliega la aplicaci√≥n a Google Cloud Run"""
    try:
        # Determinar el comando correcto de gcloud para Windows
        gcloud_cmd = "gcloud.cmd" if sys.platform == "win32" else "gcloud"
        
        # Verificar si gcloud est√° instalado
        print("   üîç Verificando instalaci√≥n de gcloud...")
        gcloud_check = subprocess.run(
            [gcloud_cmd, "--version"],
            capture_output=True,
            text=True,
            timeout=5,
            shell=True if sys.platform == "win32" else False
        )
        
        if gcloud_check.returncode != 0:
            print("   ‚ö†Ô∏è gcloud CLI no est√° correctamente configurado.")
            print(f"   üìÑ Error: {gcloud_check.stderr}")
            return
        
        print("   ‚úÖ gcloud CLI encontrado")
        
        # Verificar autenticaci√≥n
        print("   üîê Verificando autenticaci√≥n de gcloud...")
        auth_check = subprocess.run(
            [gcloud_cmd, "auth", "list", "--filter=status:ACTIVE", "--format=value(account)"],
            capture_output=True,
            text=True,
            timeout=10,
            shell=True if sys.platform == "win32" else False
        )
        
        if not auth_check.stdout.strip():
            print("   ‚ö†Ô∏è No hay cuenta de Google Cloud autenticada.")
            print("   üí° Ejecuta: gcloud auth login")

            return
        
        print(f"   ‚úÖ Autenticado como: {auth_check.stdout.strip()}")
        
        # Commit cambios a Git primero
        print("   üì¶ Sincronizando con GitHub...")
        
        # 1. Add
        subprocess.run(["git", "add", "."], cwd=os.path.dirname(__file__) or ".", check=False)
        
        # 2. Commit
        commit_res = subprocess.run(
            ["git", "commit", "-m", "sync: Actualizaci√≥n autom√°tica de datos y modelos"],
            cwd=os.path.dirname(__file__) or ".",
            check=False,
            capture_output=True,
            text=True
        )
        if commit_res.returncode == 0:
             print("   ‚úÖ Commit creado.")
        else:
             if "nothing to commit" in commit_res.stdout:
                 print("   ‚ÑπÔ∏è  Nada que commitear.")
             else:
                 print(f"   ‚ö†Ô∏è Warning en commit: {commit_res.stdout}")

        # 3. Push (Try only)
        print("   ‚¨ÜÔ∏è  Subiendo cambios (git push)...")
        push_res = subprocess.run(
            ["git", "push"], 
            cwd=os.path.dirname(__file__) or ".", 
            check=False,
            capture_output=True,
            text=True
        )

        if push_res.returncode == 0:
            print("   ‚úÖ Cambios pusheados a GitHub correctamente.")
        else:
            print("   ‚ùå Error al pushear a GitHub (Tu rama local est√° adelante/divergente).")
            print("   ‚ö†Ô∏è  No te preocupes, el DEPLOY a Cloud Run continuar√° igual.")
            print(f"   üìÑ Detalle: {push_res.stderr}")
        
        # Desplegar a Cloud Run
        print("   üöÄ Desplegando a Cloud Run...")
        
        # Cargar API key desde .env para Cloud Run
        from dotenv import load_dotenv
        load_dotenv()
        gemini_key = os.getenv('GEMINI_API_KEY', '')
        
        # Ofuscar para log
        key_preview = f"{gemini_key[:10]}...{gemini_key[-4:]}" if len(gemini_key) > 14 else "***"
        print(f"   üîê Configurando GEMINI_API_KEY: {key_preview}")
        
        result = subprocess.run(
            [
                gcloud_cmd, "run", "deploy", "pista-inteligente",
                "--source", ".",
                "--region", "us-central1",
                "--allow-unauthenticated",
                "--set-env-vars", f"GEMINI_API_KEY={gemini_key}",
                "--quiet"
            ],
            cwd=os.path.dirname(__file__) or ".",
            capture_output=True,
            text=True,
            timeout=300,  # 5 minutos max
            shell=True if sys.platform == "win32" else False
        )
        
        if result.returncode == 0:
            print("   ‚úÖ DEPLOY EXITOSO a Cloud Run!")
            # Extraer URL del output
            for line in result.stdout.split('\n'):
                if 'https://' in line:
                    print(f"   üåê URL: {line.strip()}")
                    break
        else:
            print("   ‚ùå Error en deploy a Cloud Run:")
            print(f"   üìÑ STDOUT: {result.stdout}")
            print(f"   üìÑ STDERR: {result.stderr}")
            
    except FileNotFoundError:
        print("   ‚ö†Ô∏è gcloud CLI no encontrado en el PATH del sistema.")
        print("   üí° Tip: Instala Google Cloud SDK o verifica tu PATH")
    except subprocess.TimeoutExpired:
        print("   ‚ö†Ô∏è Timeout en el deploy. El proceso tom√≥ demasiado tiempo.")
    except Exception as e:
        print(f"   ‚ùå Error inesperado en deploy: {type(e).__name__}: {e}")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Sistema de Sincronizaci√≥n H√≠pica Inteligente')
    parser.add_argument('--force', action='store_true', help='Forzar re-procesamiento completo')
    parser.add_argument('--no-deploy', action='store_true', help='Saltar paso de despliegue')
    
    args = parser.parse_args()
    
    main(force_sync=args.force)

